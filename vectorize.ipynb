{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a list of words.\n",
    "        :param words: List of words to be converted into OHE tensors.\n",
    "        \"\"\"\n",
    "        self.words = [word.lower() for word in words if isinstance(word, str)]\n",
    "        \n",
    "        # Vocabulary setup\n",
    "        self.vocab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        self.vocab_size = len(self.vocab) + 1  # +1 for unknown characters\n",
    "        self.ctoi = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        \n",
    "        # One-Hot Encoding Matrix\n",
    "        self.create_OHE()\n",
    "    \n",
    "    def create_OHE(self):\n",
    "        \"\"\"Creates a One-Hot Encoding matrix for the vocabulary.\"\"\"\n",
    "        self.OHE = torch.eye(self.vocab_size)  # Identity matrix for one-hot encoding\n",
    "    \n",
    "    def get_OHE(self, word):\n",
    "        \"\"\"Converts a word into a one-hot encoding tensor.\"\"\"\n",
    "        emb = [self.OHE[self.ctoi.get(char, self.vocab_size - 1)] for char in word]\n",
    "        return torch.stack(emb) if emb else torch.zeros((1, self.vocab_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns One-Hot Encoding tensor for a word.\"\"\"\n",
    "        return self.get_OHE(self.words[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads sequences in a batch to the max length in the batch.\"\"\"\n",
    "    max_len = max(word.shape[0] for word in batch)\n",
    "    vocab_size = batch[0].shape[1]\n",
    "    pad_tensor = torch.zeros((max_len, vocab_size))\n",
    "    \n",
    "    padded_batch = [torch.cat((word, pad_tensor[:max_len - word.shape[0]]), dim=0) for word in batch]\n",
    "    \n",
    "    return torch.stack(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, vocab_size=27, emb_dim=50, num_epochs=15, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        self.vocab_size = len(self.vocab) + 1\n",
    "        self.ctoi = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm1 = nn.LSTM(input_size=self.vocab_size, hidden_size=emb_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)  \n",
    "\n",
    "        \n",
    "        self.create_OHE()\n",
    "\n",
    "    def create_OHE(self):\n",
    "        \"\"\"Creates a One-Hot Encoding matrix for the vocabulary.\"\"\"\n",
    "        self.OHE = torch.zeros((self.vocab_size, self.vocab_size))\n",
    "        for i in range(self.vocab_size):\n",
    "            self.OHE[i, i] = 1\n",
    "\n",
    "    def get_OHE(self, word):\n",
    "        \"\"\"Converts a word into a one-hot encoding tensor.\"\"\"\n",
    "        emb = [self.OHE[self.ctoi.get(char, self.vocab_size - 1)] for char in word]\n",
    "        return torch.stack(emb)\n",
    "    \n",
    "    def fit(self, batched_data):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x1, x2, target_batch in batched_data:\n",
    "                x1, x2, target_batch = x1.to(self.device), x2.to(self.device), target_batch.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.forward(x1, x2)\n",
    "                loss = self.loss_fn(outputs, target_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"{epoch+1}/{self.num_epochs} - Loss: {epoch_loss / len(batched_data)}\")\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        out1, _ = self.lstm1(x)\n",
    "        out2, (hn, _) = self.lstm2(out1)\n",
    "        return hn.squeeze(0) \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        emb1 = self.get_embedding(x1)\n",
    "        emb2 = self.get_embedding(x2)\n",
    "\n",
    "        diff = emb1 - emb2\n",
    "        squared_norm = torch.sum(diff ** 2, dim=1, keepdim=True)\n",
    "\n",
    "        out = torch.sigmoid(self.fc(squared_norm))\n",
    "        return out\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        torch.save(self.state_dict(), model_name)\n",
    "        print(f\"Model saved to {model_name}\")\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        self.load_state_dict(torch.load(model_name))\n",
    "        self.eval()  # Set the model to evaluation mode after loading\n",
    "        print(f\"Model loaded from {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./saved_model_2/char2vec.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_27960\\1506751135.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (lstm1): LSTM(27, 50, batch_first=True)\n",
       "  (lstm2): LSTM(50, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (loss_fn): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = CustomModel() \n",
    "loaded_model.load_model(\"./saved_model_2/char2vec.pth\")\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./datasets/dict.csv\")\n",
    "words = dataset[\"word\"]\n",
    "words = list(set(words.to_numpy()))\n",
    "words = [item for item in words if isinstance(item, str) and len(item)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(words)\n",
    "dataloader = DataLoader(dataset, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(dataloader):\n",
    "    batch_embds = loaded_model.get_embedding(batch.to(loaded_model.device))\n",
    "    if index == 0:\n",
    "        embds = batch_embds  \n",
    "    else:\n",
    "        embds = torch.cat((embds, batch_embds), dim=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41145, 50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = nmslib.init(method=\"hnsw\", space=\"cosinesimil\")\n",
    "index.addDataPointBatch(embds.detach().cpu())\n",
    "index.createIndex({'post': 2}, print_progress=False)\n",
    "\n",
    "# Save index and word list\n",
    "index.saveIndex(\"./saved_model_2/word_index.bin\", save_data=True)\n",
    "np.save(\"./saved_model_2/word_list.npy\", words)  # Save word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = nmslib.init(method=\"hnsw\", space=\"cosinesimil\")\n",
    "index.loadIndex(\"./saved_model_2/word_index.bin\", load_data=True)\n",
    "words = np.load(\"./saved_model_2/word_list.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = loaded_model.get_embedding(loaded_model.get_OHE(\"aple\").to(loaded_model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ale', 'hale', 'Yale', 'yale', 'axle', 'tale', 'whale', 'Male', 'pale', 'male', 'maple', 'aloe', 'wale', 'gale', 'kale', 'bale', 'Gale', 'ladle', 'algae', 'parole', 'dale', 'Dale', 'anole', 'apple', 'coalhole', 'yule', 'ampoule', 'Maypole', 'cable', 'vale', 'Malope', 'alcalde', 'macule', 'sale', 'Argyle', 'table', 'glare', 'parolee', 'audile', 'papule', 'gable', 'tamale', 'amble', 'blare', 'hackle', 'calycle', 'jugale', 'mule', 'hole', 'example', 'haulage', 'value', 'shale', 'phalarope', 'tole', 'haggle', 'Palmae', 'awe', 'Cakile', 'whole', 'phyle', 'dapple', 'playhouse', 'ladylove', 'mole', 'Mole', 'ramble', 'Thule', 'role', 'parable', 'Elaphe', 'gamble', 'laparocele', 'paddle', 'marble', 'allele', 'axe', 'glaze', 'ullage', 'ape', 'tackle', 'raddle', 'goalie', 'payable', 'cymule', 'palate', 'glareole', 'age', 'lame', 'hame', 'tadpole', 'abele', 'lake', 'eagle', 'amylase', 'lazuline', 'flagpole', 'blame', 'tulle', 'mallee'] [0.18107772 0.21209908 0.21947646 0.2217018  0.22289205 0.22859603\n",
      " 0.2304402  0.23138589 0.23227042 0.233329   0.23756397 0.23775071\n",
      " 0.23895073 0.23974746 0.24033582 0.2415064  0.24153763 0.25147218\n",
      " 0.255556   0.26120502 0.26218396 0.2625354  0.26560193 0.26749736\n",
      " 0.26935774 0.27018613 0.2702397  0.27072465 0.2715047  0.27764088\n",
      " 0.27787238 0.27829832 0.27856672 0.27985746 0.2824685  0.28326094\n",
      " 0.28374362 0.28505647 0.28697455 0.28700113 0.28705782 0.28707463\n",
      " 0.28779715 0.28827626 0.2889197  0.2907219  0.2918862  0.29382414\n",
      " 0.29468024 0.29507035 0.29606634 0.2967652  0.29679924 0.2977373\n",
      " 0.2989046  0.2989596  0.30059254 0.3006928  0.30185616 0.30192\n",
      " 0.30193144 0.30203527 0.3024348  0.30269235 0.3028643  0.3028643\n",
      " 0.3040145  0.30404496 0.3046633  0.30479473 0.30488354 0.30618435\n",
      " 0.3062641  0.30644858 0.30715585 0.30715996 0.30799705 0.30846983\n",
      " 0.3089441  0.30917084 0.3092373  0.30948585 0.30974382 0.3097763\n",
      " 0.31050205 0.31062806 0.3107347  0.3116681  0.31193304 0.31210333\n",
      " 0.3125896  0.31336784 0.31416917 0.31470275 0.3148139  0.31507355\n",
      " 0.31527638 0.31530488 0.31546646 0.31632096]\n"
     ]
    }
   ],
   "source": [
    "ids, distances = index.knnQuery(query_vector.detach().cpu(), k=100)\n",
    "print([words[i] for i in ids], distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
